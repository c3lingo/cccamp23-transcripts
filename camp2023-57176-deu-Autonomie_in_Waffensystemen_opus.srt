1
00:00:00,000 --> 00:00:29,000
 [Musik]

2
00:00:30,000 --> 00:00:46,000
 Genau, also ich möchte euch heute einen kleinen Einblick in die aktuellen Entwicklungen im Kontext autonomer Waffensysteme bzw. von Autonomie in Waffensystemen und militärischer KI geben.

3
00:00:46,000 --> 00:01:00,000
 Einfach weil künstliche Intelligenz und selbstlernende Systeme immer mehr Einfluss auf ganz viele gesellschaftliche Bereiche nehmen und eben nicht nur im zivilen Bereich, sondern immer mehr auch im militärischen Bereich.

4
00:01:00,000 --> 00:01:13,000
 Und genau, das bringt ganz viele Probleme mit sich hinsichtlich des Kriegskontexts und auch der Kontrolle und der Regulierung von solchen Systemen.

5
00:01:13,000 --> 00:01:24,000
 Genau, ich bin Politikwissenschaftlerin und Friedens- und Konfliktforscherin und arbeite bei PISEC an der TU Darmstadt.

6
00:01:24,000 --> 00:01:31,000
 Und genau, PISEC verbindet eben die Informatik mit der Friedens- und Sicherheitsforschung.

7
00:01:31,000 --> 00:01:49,000
 Wir haben da ein sehr interdisziplinäres Forschungsfeld des Friedens- und Konfliktforschung, Cybersicherheit und Privatheit und die Mensch-Computer-Interaktion miteinander verbindet und da Schnittpunkte untersucht.

8
00:01:56,000 --> 00:02:12,000
 Genau, und dann möchte ich einsteigen mit der Frage, was sind eigentlich autonome Waffensysteme, weil wir da schon bei einer der Hauptproblematiken sind, was den Umgang mit solchen Systemen angeht.

9
00:02:12,000 --> 00:02:29,000
 Weil es tatsächlich bis heute keine einheitlich international anerkannte Definition von autonomen Waffensystemen gibt, was unter anderem auch damit zusammenhängt, dass es noch keine vollautonomen Systeme in der engen Definition gibt.

10
00:02:29,000 --> 00:02:42,000
 Und auch einige Staaten aus einer militärstrategischen Perspektive gar nicht so ein großes Interesse daran haben, das so genau zu definieren.

11
00:02:42,000 --> 00:03:01,000
 Und häufig werden autonome Waffensysteme eben mit unbemannten Drohnen assoziiert, also zum Beispiel einer, wie man sie hier jetzt sieht. Und genau, die werden eben dann autonom oder telautonom ferngesteuert als mobile Waffensysteme.

12
00:03:01,000 --> 00:03:12,000
 Auch wenn das längst nicht das einzige Anwendungsszenario jetzt ist, aber das ist eben so das Bild, was dann meistens damit assoziiert wird.

13
00:03:12,000 --> 00:03:34,000
 Und eine sehr bekannte Definition von autonomen Waffensystemen stammt vom internationalen Komitee des Roten Kreuzes, die habe ich hier mal mitgebracht, nämlich "any weapon system with autonomy and its critical functions that can select and attack targets without human intervention".

14
00:03:34,000 --> 00:03:48,000
 Also der Begriff bezeichnet in der Definition dann Waffensysteme, die ohne menschliches Zutun entscheiden, welche Ziele sie anvisieren und dann auch angreifen.

15
00:03:48,000 --> 00:04:09,000
 Genau, häufig wird auch von autonomen Waffensystemen gesprochen, wenn es nur Teilautonome Waffensysteme sind, also zum Beispiel im Kontext von Assistenzsystemen, ferngesteuerte Drohnen, bei denen dann einige Funktionen autonom ablaufen und andere von einem Menschen kontrolliert werden.

16
00:04:09,000 --> 00:04:26,000
 Und hier ist es auch nochmal ganz wichtig, sich klar zu machen, dass es keine harte Grenze zwischen Automatisierung und Autonomisierung gibt, weil das eigentlich eher nur unterschiedlich komplexe Stufen derselben Technologie sind.

17
00:04:26,000 --> 00:04:40,000
 Und es geht dabei vielmehr darum, um die Frage, in welchem Ausmaß der Mensch noch in der Entscheidungsschleife des Systems dann drin ist.

18
00:04:40,000 --> 00:05:08,000
 Und auch wenn jetzt die vollautonomen Waffensysteme in der engen Definition noch nicht im Einsatz sind, ist es eben total wichtig, jetzt schon Regulierungsmöglichkeiten zu finden, weil die ganzen Risiken und Herausforderungen eben jetzt schon antizipiert werden müssen, damit wir eben irgendwie einen Überblick haben, was da auf uns zukommt und die Entwicklung der Technologie auch schon mit begleiten können.

19
00:05:08,000 --> 00:05:22,000
 So, jetzt, wo findet eigentlich schon ein Einsatz von teilautonomen Waffen statt? Da kommt es auch wieder drauf an, welche Definition man jetzt anlegt.

20
00:05:22,000 --> 00:05:30,000
 Je nach Definition können aber auf jeden Fall schon autonome oder teilautonome Systeme beobachtet werden.

21
00:05:30,000 --> 00:05:38,000
 Ein Beispiel ist das Beispiel auf der linken Seite, das ist das israelische System Hapi.

22
00:05:38,000 --> 00:05:49,000
 Das kann zum Beispiel autonom über ein Gebiet kreisen und auch gegnerische Luftangriffe abwehren.

23
00:05:49,000 --> 00:06:02,000
 Und im Kontext des russischen Eingriffs auf die Ukraine haben am Anfang des Krieges vor allem konventionelle Waffen dominiert.

24
00:06:02,000 --> 00:06:23,000
 Es gibt aber auch das KI-Erkennungssystem für Raketenabwehr und mittlerweile gibt es auch immer mehr Berichte über den Einsatz von Drohnen, zum Beispiel die sogenannte Switchblade-Drohne hier auf der rechten Seite, die auch über autonome Funktionen verfügt.

25
00:06:23,000 --> 00:06:32,000
 In jedem Fall gehören unbemannte Drohnen auch mit autonomen Funktionen längst schon zum militärischen Inventar.

26
00:06:32,000 --> 00:06:43,000
 Und auch wenn heute noch ein Mensch den finalen Schießbefehl geben muss, aber auf jeden Fall ist da schon einiges im Einsatz.

27
00:06:43,000 --> 00:06:53,000
 So, die Frage, was ist denn jetzt eigentlich so problematisch am Einsatz autonomer Waffen?

28
00:06:53,000 --> 00:07:13,000
 Autonome Waffensysteme werfen aus ganz vielen verschiedenen Perspektiven Probleme und Herausforderungen mit sich, unter anderem aus technischer, völkerrechtlicher, sicherheitspolitischer, ethischer und humanitärer Perspektive.

29
00:07:13,000 --> 00:07:34,000
 Und hier ist noch ganz viel Forschung nötig, damit verschiedene gesellschaftliche Akteure aus Zivilgesellschaft, Wissenschaft, von Staaten, aber auch aus dem Militär und aus der Industrie zusammen diskutieren können und da Lösungen finden können.

30
00:07:34,000 --> 00:07:54,000
 Wenn man sich jetzt zum Beispiel die rechtlich-technischen Probleme anschaut, dann ergeben sich zum Beispiel im Kontext der Genfer Konventionen ganz wichtige Fragen.

31
00:07:54,000 --> 00:08:07,000
 Wer trägt zum Beispiel die juristische Verantwortung für die Handlung autonomer Systeme? Und sind die bestehenden Gesetze eigentlich schon ausreichend oder brauchen wir da ganz neue?

32
00:08:07,000 --> 00:08:17,000
 Und dann auch so was, wie können eigentlich zentrale Prinzipien des Völkerrechts in Softwarecode gefasst werden, also ist das überhaupt möglich?

33
00:08:17,000 --> 00:08:32,000
 Und eine der zentralsten Fragen im rechtlichen Kontext ist, ob autonome Waffensysteme in der Lage sein werden, in Kriegskontexten Zivilisten von Konvertanten unterscheiden zu können.

34
00:08:32,000 --> 00:08:47,000
 Dann mehr aus technischer Perspektive die Frage, wie gehen wir mit dem Blackbox-Problem um. Mit immer mehr Autonomie wächst eben auch das Risiko für unerwartetes Verhalten von Systemen und technische Schwachstellen.

35
00:08:47,000 --> 00:08:59,000
 Und deswegen stellt sich da auch die Frage, ob und wie aus technischer Perspektive die Entscheidungen einer KI irgendwie transparenter gemacht werden können.

36
00:08:59,000 --> 00:09:09,000
 Und das Problem ist auch total relevant für teilautonome Systeme, bei denen sich noch ein Mensch in der Entscheidungsschleife befindet.

37
00:09:09,000 --> 00:09:14,000
 Da gibt es zum Beispiel Studien zum sogenannten Automation Bias heißt es.

38
00:09:14,000 --> 00:09:29,000
 Die zeigen, dass Menschen Vorschläge von KI-gesteuerten Assistenzsystemen in den allermeisten Fällen einfach annehmen, weil sie gar nicht mehr nachvollziehen können, wie diese Entscheidung jetzt zustande gekommen ist.

39
00:09:29,000 --> 00:09:38,000
 Und deswegen kann man auch bei teilautonomen Systemen von dem Kontrollverlust des Menschen sprechen.

40
00:09:38,000 --> 00:09:48,000
 Dann aus ethisch-humanitärer Perspektive führt der Einsatz von KI-Waffen eben zu einer weiteren Entmenschlichung der Kriegsführung.

41
00:09:48,000 --> 00:09:59,000
 Wenn Entscheidungen über Leben und Tod delegiert werden an eine KI, dann sind Menschen nur noch Datenmuster und Objekte.

42
00:09:59,000 --> 00:10:05,000
 Und das würde eine ganz krasse Verletzung der Menschen darstellen.

43
00:10:05,000 --> 00:10:12,000
 Deswegen ist es gerade aus ethischer Perspektive total wichtig, dass immer ein Mensch auch in der Entscheidungsschleife bleibt.

44
00:10:12,000 --> 00:10:25,000
 Und deswegen hat 2013 die NGO Artikel 36 das sogenannte Konzept der sogenannten Meaningful Human Control eingebracht.

45
00:10:25,000 --> 00:10:29,000
 Also eine bedeutsame menschliche Kontrolle.

46
00:10:29,000 --> 00:10:40,000
 Und dieses Konzept ist sehr erfolgreich geworden. Mittlerweile wird es von sehr vielen internationalen Organisationen und auch sehr vielen Staaten immer wieder aufgegriffen.

47
00:10:40,000 --> 00:10:51,000
 Allerdings ist noch nicht so ganz klar, wie diese bedeutsame menschliche Kontrolle aussehen soll und wie sie auch praktisch umgesetzt werden könnte.

48
00:10:51,000 --> 00:11:00,000
 Und da ist auch noch ganz viel Forschung, gerade im Kontext der Mensch-Maschine-Interaktion, nötig.

49
00:11:00,000 --> 00:11:20,000
 Und dann gibt es noch eine weitere Perspektive auf autonome Waffensysteme, die im Vergleich zu technischen und zu rechtlichen Aspekten in der internationalen Debatte sehr wenig Gewicht hat und trotzdem unglaublich relevant ist.

50
00:11:20,000 --> 00:11:31,000
 Und zwar die Frage, wie objektiv ist eigentlich KI und welche Auswirkungen hat es in der Anwendung von Waffensystemen?

51
00:11:31,000 --> 00:11:46,000
 Es ist nämlich immer noch ein sehr weit verbreiter Irrglaub, dass KI objektivere Entscheidungen treffen können als Menschen, keine Vorurteile hat und sowieso weniger Fehler macht.

52
00:11:46,000 --> 00:11:53,000
 Das Gegenteil ist der Fall. Algorithmen und KI sind als andere als neutral und objektiv.

53
00:11:53,000 --> 00:12:06,000
 Einfach weil diese Systeme von Menschen konzipiert, entwickelt und angewendet werden und deswegen spiegeln sie auch diese Vorurteile, Fehler und Verzerrungen.

54
00:12:06,000 --> 00:12:18,000
 Das fängt beispielsweise schon bei den Trainingsdaten an. Die Datengrundlage zum Trainieren von Algorithmen sind eben als andere als eine Sammlung objektiver Fakten.

55
00:12:18,000 --> 00:12:25,000
 Und besonders in zivilen Anwendungen wird das schon sehr deutlich und da gibt es zahlreiche Beispiele.

56
00:12:25,000 --> 00:12:45,000
 Beispielsweise gibt es Studien, die zeigen, dass Gesichtserkennungssoftware, People of Color, generell schlechter erkennt als weiße Menschen, Frauen schlechter als Männer und besonders Women of Color, also schwarze Frauen, sehr überdurchschnittlich häufig falsch erkannt werden.

57
00:12:45,000 --> 00:13:05,000
 Und ähnliches gilt auch für Stimmerkennungssoftware. Und das wirft eben die total drängende Frage auf, wie geeignet diese Trainingsdaten dann sind, gerade wenn man sie in so kritischen und sensiblen Bereichen einsetzt wie in militärischen Anwendungen.

58
00:13:05,000 --> 00:13:18,000
 In Kriegskontexten kommen dann neben solcher Einschreibungen vorbelasteter Daten noch sogenannte Signifier ins Spiel.

59
00:13:18,000 --> 00:13:37,000
 Die werden in Kriegskontexten benutzt, um Menschen im sogenannten Target Profiling einzuordnen, weil eben nach Genfer Konvention nur eine Person angegriffen werden darf, die eindeutig als kämpfend und nicht als zivil eingeordnet wurde.

60
00:13:37,000 --> 00:13:56,000
 Und deswegen ist diese Unterscheidung so zentral. Und in Bezug auf den Einsatz semi-autonomer Thron gibt es schon Beispiele, dass zum Beispiel junge Männer in Kriegskontexten fast immer als Kämpfer eingestuft werden.

61
00:13:56,000 --> 00:14:12,000
 Und das impliziert einfach, dass jede Personengruppe, deren Gender zum Beispiel in einem Zielprofil enthalten ist, einem viel größeren Risiko automatisch ausgesetzt wäre.

62
00:14:12,000 --> 00:14:32,000
 Und andere Berichte über bewaffnete Throneneinsätze zeigen auch, dass neben Gender auch Alter und religiöse Handlungen eine Rolle spielen, also zum Beispiel das Beten in Gruppen eben als Indikatoren dafür, ob Personen in Kriegsgebieten angegriffen werden.

63
00:14:32,000 --> 00:14:46,000
 Und ja, ein KI in Waffensystemen würde solche Entwicklungen wahrscheinlich weiter verstärken und vor allem auch nicht hinterfragen.

64
00:14:46,000 --> 00:15:00,000
 Dann will ich jetzt zum Schluss noch ganz kurz darauf eingehen, warum eine Regulierung von autonomen Waffensystemen eigentlich so schwierig ist, die öffentliche Diskussion über Autonomie in Waffensystemen,

65
00:15:00,000 --> 00:15:26,000
 vor allem dem zivilgesellschaftlichen Prozess der Campaign to Stop Killer Robots zu verdanken, sieht man hier rechts auf dem Bild. Es ist ein internationales NGO-Netzwerk, die seit schon über zehn Jahren die Entmenschlichung durch automatisierte Entscheidungsfindungen anprangern und eben ein weltweites Verbot fordern.

66
00:15:26,000 --> 00:15:46,000
 Und die waren auch sehr erfolgreich. Zum Beispiel haben sie dafür gesorgt, dass bei den Vereinten Nationen eine Expertengruppe eingesetzt wurde, die "Group of Governmental Experts" und die verhandeln offiziell seit 2017 über die Regulierung von autonomen Waffensystemen.

67
00:15:46,000 --> 00:15:58,000
 Bisher konnten sie sich leider noch nicht auf sehr viel einigen, bis auf ein paar eher vage formulierte Leitlinien.

68
00:15:58,000 --> 00:16:22,000
 Eine sehr hohe Hürde für die Regulierung ist seit Jahren die Einigung auf eine einheitliche Definition. Da wären wir wieder beim Anfangsproblem, weil eine einheitliche Definition sehr lange als notwendige Grundlage gesehen wurde, für zum Beispiel einen möglichen Rüstungskontrollvertrag.

69
00:16:22,000 --> 00:16:33,000
 Und dass die Regulierung von autonomen Waffensystemen so schwierig ist, hat mehrere Gründe. Ich habe jetzt mal hier drei wichtige aufgeführt.

70
00:16:33,000 --> 00:16:48,000
 Zum einen sind autonome Waffen keine klar abgrenzbare Waffenkategorie, wie das zum Beispiel bei Nuklearwaffen der Fall ist, die man genau beschreiben kann und dann regulieren kann.

71
00:16:48,000 --> 00:16:59,000
 Vielmehr ist Autonomie eine Eigenschaft, die in unterschiedliche Systeme integriert werden kann, sowohl in ziviler als auch in militärische.

72
00:16:59,000 --> 00:17:11,000
 Und deswegen wird mittlerweile auch immer häufiger von Autonomie in Waffensystemen gesprochen, statt von autonomen Waffensystemen, weil es diesen Aspekt eben deutlicher macht.

73
00:17:11,000 --> 00:17:21,000
 Dann ein weiterer Punkt ist, dass im klassischen Ansatz der Rüstungskontrolle oft Waffensysteme quantitativ beschränkt werden.

74
00:17:21,000 --> 00:17:29,000
 Dieser Ansatz greift hier aber auch nicht, weil wie beschränkt man quantitativ Software?

75
00:17:29,000 --> 00:17:43,000
 Deswegen braucht es da auch neue qualitative Formen der Rüstungskontrolle, die beispielsweise dann mehr den Anwendungskontext von Waffensystemen in den Blick nimmt.

76
00:17:43,000 --> 00:17:56,000
 Der dritte Punkt, den ich hier jetzt noch aufgeführt habe, ist, dass die Auswirkungen von Autonomie in Waffensystemen bisher noch nicht so sichtbar sind, wie das bei anderen Waffen der Fall ist.

77
00:17:56,000 --> 00:18:03,000
 Und deswegen kann man sich auch nicht so einfach an anderen Prozessen orientieren.

78
00:18:03,000 --> 00:18:16,000
 Genau. Und das sind jetzt nur ausgewählte Punkte, aber das alles pöte dazu, dass die Regulierung solcher Systeme enorm erschwert wird.

79
00:18:16,000 --> 00:18:29,000
 Und dass auch die Verhandlung im zuständigen Arbeitsgremium bei den Vereinten Nationen da seit Jahren eigentlich kaum vorankommt und auch nicht zu neuen Ergebnissen kommt.

80
00:18:29,000 --> 00:18:48,000
 Der einzige Konsens, wenn man es so nennen kann, ist derzeit, dass im Kontext von Autonomie in Waffensystemen eben eine hinreichende Form von menschlicher Kontrolle da sein soll.

81
00:18:48,000 --> 00:18:54,000
 Aber wie die jetzt genau aussehen soll, darüber gibt es noch keine Einigkeit.

82
00:18:54,000 --> 00:19:00,000
 Ja, dann wäre ich am Ende.

83
00:19:00,000 --> 00:19:02,000
 Vielen Dank.

84
00:19:02,000 --> 00:19:04,000
 Vielen Dank.

85
00:19:05,000 --> 00:19:10,000
 [Musik]

