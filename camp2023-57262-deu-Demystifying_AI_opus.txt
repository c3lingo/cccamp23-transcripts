 [Musik]
 Gut, dann fange ich ohne Folien an, geht auch.
 Genau, also ich habe mir nochmal vorgenommen, darüber zu reden, wie lernen Maschinen eigentlich.
 Wir haben gerade durch Chat-GPT sehr viel mit dem Thema zu tun.
 Und ich dachte, das ist nochmal eine ideale Gelegenheit, einen Schritt zurückzugehen und nochmal zu gucken, was passiert da eigentlich im Hintergrund.
 Wie ihr gerade schon erfahren habt, mag ich gerne Mathe und ich habe mich auch lange und ausgiebig damit beschäftigt.
 Ich arbeite zurzeit bei RSA Labs und davor habe ich 10 Jahre Mathe in unterschiedlichen Bereichen gemacht
 und habe mich auch unter anderem mit dem Thema maschinelles Lernen bzw. Deep Learning beschäftigt.
 Und habe dadurch einfach wissend angehäuft, was ich gerne teilen will, sodass dieses Wissen sich verbreitet
 und mehr Leute sich diese ganzen Anwendungen auch einfach nochmal kritisch angucken können und eigentlich verstehen,
 was ist das eigentlich und was hat das eigentlich mit diesem ganzen Lernen zu tun.
 Genau, ich denke durch den aktuellen Hype, den wir gerade sehen, durch die unterschiedlichen KI-Anwendungen, tendieren wir dazu, KI zu überschätzen.
 Und das führt zum einen dann dazu, dass KI ein Passwort ist, was überall auftaucht.
 Wir können noch nicht mehr eine normale Kaffeemaschine kaufen, ohne dass da KI drin steckt, was totaler Quatsch ist,
 weil im Endeffekt, da ist eine Kaffeemaschine abgebildet, die man tatsächlich kaufen kann.
 Und die KI, die da drin steckt, ist halt einfach angewandte Statistik.
 Und das ist im Prinzip auch das Geheimnis hinter der KI. Also es ist wirklich einfach erstmal nur Statistik.
 Das zweite Problem ist, die Anwender haben ein Interesse daran, die Hersteller haben ein Interesse daran, dieses Thema als etwas sehr Komplexes darzustellen,
 sodass es natürlich schwierig ist, das Ganze nachzubauen und so weiter und so fort.
 Und ich meine, auch heute haben wir wieder die Situation, ich habe mich sehr lange mit dem Thema beschäftigt und in Mathe promoviert,
 aber man braucht keinen Doktor in Mathe, um dieses Thema zu verstehen. Das soll ganz klargestellt werden an dieser Stelle.
 Ein weiteres Problem ist, wir sehen tatsächlich die beeindruckenden Beispiele auf Twitter oder Social Media
 und dann denken wir, boah, jetzt kann die Maschine auf einmal alles und wir sind irgendwie alle überflüssig.
 Und das ist definitiv nicht der Fall. Und ich hoffe, ihr stimmt mir am Ende meines Talks dann noch zu.
 Und jetzt gerade durch diese aktuellen Anwendungen im Themenbereich Sprache, haben wir uns glaube ich auch so ein Stück weit selber gehackt,
 weil jetzt haben wir eine Maschine, die kann sprechen und jetzt denken wir, hey, das kann sprechen, das muss intelligent sein.
 Wenn ich das jetzt zum Beispiel mit meinem Hund vergleiche, der kann nicht sprechen und dem spreche ich aber eine gewisse Intelligenz erstmal ab.
 Und dann habe ich eine Maschine, die macht irgendwas, generiert irgendwie Sprache, die irgendwie gut aussieht
 und natürlich muss dadurch dann natürlich eine hohe Intelligenz da sein, dass es überhaupt sprechen kann.
 Genau und damit würde ich jetzt erst nochmal den Schritt zurückgehen. Was ist denn eigentlich KI?
 Es gibt da drei Themenbereiche. Der große überspannende Bereich ist künstliche Intelligenz.
 Und das ist definiert und das ist auch schon relativ lange so, dass es Computer sind, die imitieren Denken und menschliches Denken und Verhalten.
 Das ist erstmal unabhängig davon, was dann darunter steht.
 Und dann haben wir einen Themenbereich, der nennt sich maschinelles Lernen und das sind statistische Algorithmen,
 die diese künstliche Intelligenz irgendwie darstellen.
 Und die basieren tatsächlich erstmal sehr viel auf Statistik und davon haben wir dann eine Unterkategorie und das ist das sogenannte Deep Learning.
 Und das Deep Learning ist ein Teilbereich des maschinellen Lernens.
 Das ist das Lernen mit tiefen, mit vielen Layern, sogenannten Layern unter im Umgang dann halt auch entsprechend neuronalen Netzen.
 Und ich probiere in meinem Talk, also ich werde jetzt auf den Bereich Machine Learning eingehen und auch ein bisschen auf den Bereich Deep Learning
 und probiere so ein bisschen auch den Unterschied zwischen diesen beiden Feldern aufzuzeigen.
 Genau, also jetzt gerade haben wir zwei unterschiedliche große Bereiche der KI. Wir haben manchmal eine spezialisierte KI, die kann im Prinzip einfach zwei Probleme lösen.
 Und das eine ist eine Klassifikation, das hier dargestellt durch einen Hund, der von einem Muffin unterschieden werden soll,
 was tatsächlich relativ schlecht funktioniert, weil das Ding drei schöne schwarze Punkte hat und dadurch ist es tatsächlich schwierig.
 Für uns tatsächlich sehr, sehr einfach, aber genau das nebenbei.
 Und das zweite Themenbereich oder das zweite Problem, was spezialisierte KI lösen kann, ist Regression.
 Das heißt die Vorhersage in einen Bereich, wofür es noch keine Daten gibt.
 Es lernt aus historischen Daten und macht dann eine Vorhersage in einen Bereich, wo es keine Daten gibt.
 Das ist der Teilbereich spezialisierte KI und dann gibt es jetzt das neue, die generative KI.
 Die generative KI ist tatsächlich, eigentlich löst es genau die gleichen Probleme.
 Weil jetzt gerade die generative KI im Bereich Sprache guckt sich irgendwie an, was ist das nächste, beste Wort, was ich vorhersagen kann.
 Das ist eine Regression im Endeffekt und es hat aber auch einen Klassifikationscharakter, in dem es halt guckt,
 inwieweit sind denn die Worte miteinander ähnlich.
 Im Gegensatz zur spezialisierten KI hat die generative KI aber viel mehr Möglichkeiten, gute Antworten zu geben.
 Eine Sprache kann in vielen unterschiedlichen Bereichen überzeugend sein und kann auch ganz unterschiedlich formuliert sein,
 sodass der Anwender denkt, hey, das ist gut, während ich im Bereich spezialisierte KI ganz klar sagen kann, das ist ein Muffin und das ist ein Hund.
 Und ich glaube, dass dieser Unterschied auch schon mal dazu beiträgt, dieses ganze Themenfeld zu überschätzen.
 Also dann gibt es, ich gucke jetzt in den Bereich spezialisierte KI und in diesem Bereich gibt es drei unterschiedliche Anwendungen oder drei unterschiedliche Arten.
 Das eine ist das supervised learning.
 Beim supervised learning sind die Label von meinem Daten, also ich brauche ja immer ganz viele Daten, um das zu lernen, die Label von meinen Daten sind bekannt.
 Das heißt, ich weiß, das Bild ist ein Hund, das Bild ist ein Muffin.
 Und dazu steht im Gegensatz, dass an supervised learning dafür sind die Labels unbekannt.
 Also ich kenne die einfach nicht hier dargestellt mit Pizza, Bier und Burger.
 Und in diesem Fall probieren Algorithmen Gemeinsamkeiten herauszufinden aus den Daten und die dann zusammen in eine Klasse zu packen.
 Und der dritte Anwendungsbereich, ja gut, um jetzt für das unsupervised learning noch ein Beispiel zu nennen, ein recommender System ist zum Beispiel ein unsupervised learning.
 Also der Amazon Algorithmus, der euch immer vorschlägt, kauft auch nochmal das.
 Der probiert genau eben das zu tun.
 Er probiert zu gucken, Leute, die das gekauft haben, haben auch das gekauft.
 Aber die wissen das am Ende nicht.
 Also probieren sie es erstmal einfach nur vorzuschlagen und in einigen Fällen klappt das ja auch.
 Der dritte Teilbereich im Maschinenlernen ist das sogenannte reinforcement learning.
 Beim reinforcement learning, das ist ein bisschen anders, weil man hat einen Agenten, der in einer Umgebung agiert und er probiert ein Verhalten zu lernen.
 Im Endeffekt ist es halt aber trotzdem ähnlich zum supervised learning, weil ich weiß ja ungefähr, wie dieses Verhalten sein soll.
 Als Beispiel AlphaGo.
 AlphaGo ist ein Algorithmus, der von Google entwickelt wurde, um Go zu spielen.
 Und ich weiß ganz klar, wenn ich ein Spiel gewinne, dann war das eine gute Strategie, die ich gelernt habe oder die ich angewandt habe.
 Wenn ich ein Spiel verliere, dann war es eine schlechte Strategie.
 Der Unterschied hier ist, ich kann halt die Umgebung, in der dieser Agent unterwegs ist, abbilden.
 Und dadurch habe ich eine Möglichkeit mit den Daten, um dieses Problem zu lernen, selber zu erzeugen.
 Also das ist in den häufigsten Fällen der Fall, dass ich eben diese Umgebung tatsächlich in irgendeiner Art und Weise zur Verfügung habe.
 Und alles zusammen genommen ist es halt immer das Ziel, eine mathematische Funktion zu lernen, von einem Input, den dieses Modell bekommt, hin zu einem Output.
 Genau, und das Ganze wird halt erreicht durch Modelloptimierung.
 Also ich habe ein Modell, das ist halt im Endeffekt die KI, die füttere ich mit Daten.
 Und dieses Modell lernt dann Parameter, was wir im weiteren Verlauf auch nochmal sehen würden, wie das genau funktioniert.
 Und das tut es halt, indem es halt ein sogenanntes Training durchläuft.
 Also wir haben diese drei Bereiche, wir brauchen einmal Daten, um zu trainieren, davon brauchen wir sehr viele.
 Und um das entsprechend gut zu lernen, müssen diese Daten halt auch gut gelabelt sein.
 Dann im nächsten Schritt machen wir ein Training, dafür haben wir unterschiedliche Dinge, die wir brauchen.
 Wir müssen uns ein Modell raussuchen, vielleicht müssen wir zum Modell auch noch eine Architektur festlegen.
 Dann brauchen wir eine Lossfunktion, das ist problemabhängig.
 Und dann brauchen wir auch noch ein Optimierungsalgorithmus.
 Das sind aber alles Sachen, die ich in einer gewissen Art und Weise wählen kann.
 Also das ist nicht mathematisch festgeschrieben, wie mein Modell aussehen muss, damit ich die besten Ergebnisse erzeugen kann.
 Also von der mathematischen Perspektive her ziemlich unbefriedigend, weil ich das nicht beweisen kann, was ist tatsächlich gut, was ist schlecht.
 Aber halt auf der anderen Seite gibt es sehr viel Raum, um das halt entsprechend auszuprobieren.
 Dann mache ich mathematische Optimierung.
 Das ist auch ein Algorithmus, der zwar festgelegt ist und mathematisch gut erforscht ist,
 aber um tatsächlich gute Ergebnisse wieder zu produzieren, habe ich da auch wieder Freiheitsgrade, die ich wählen kann und wo ich irgendwie gucken kann und mit rumspielen kann.
 Und im letzten Schritt möchte ich halt nicht nur ein Modell haben, was gut funktioniert,
 sondern ich möchte ein Modell haben, was gut auf noch nicht gesehene Daten funktioniert.
 Und wenn ich jetzt einen großen Trainingsdatensatz habe und mein Modell funktioniert darauf,
 kann es ja auch sein, dass es halt entsprechende Daten gibt, worauf mein Modell überhaupt nicht mehr funktioniert.
 Und das ist genau das, worum ich mich kümmern muss. Also ich muss gucken, wie gut generalisiert mein Modell.
 Und das tue ich einfach durch einen Trick, indem ich nicht alle Daten, die ich zur Verfügung habe, zum Training benutze, sondern ich lasse einfach einen Teil raus und auf den teste ich.
 Dafür muss ich aber auch sicher sein, dass in meinen Testdaten genau das Gleiche drin ist wie in meinen Trainingsdaten.
 Also wenn ich die einfach nur zufällig wähle, kann es halt total in die Hose gehen, weil meine Testdaten gar nicht meinen Trainingsdatensatz abbilden.
 Das sind alles Sachen, die man irgendwie im Hinterkopf haben muss.
 Genau und ich würde jetzt einmal da noch tiefer reingehen und das alles noch genauer erklären.
 Und zwar würde ich das am Beispiel supervised learning machen. Wie sieht das Ganze dann am Ende aus? Also wie sieht das Training aus?
 Ich muss erstmal anfangen, ich habe hier oben Inputdaten, die haben ein Label, das ist für das supervised learning entsprechend wichtig.
 Und erstmal muss ich anfangen, meine Daten überhaupt zu cleanen. Also alles, von dem ich weiß, das ist irgendwie Quatsch, das muss da erstmal raus.
 Dann muss ich eigentlich auch im Prinzip sicher sein, dass meine Labels entsprechend auch richtig und gut sind.
 Aber das ist tatsächlich schwierig. Also das kann sehr schwierig sein und dafür braucht man meistens auch einfach nochmal menschliche Expertise, um das nachzubauen.
 Im nächsten Schritt muss ich diese Daten vorverarbeiten. Gucken wir uns später genau an, was das bedeutet.
 Dann wähle ich ein Modell, füttere meine Daten, die ich vorverarbeitet habe da rein, kriege eine Vorhersage von was auch immer.
 Und dann gucke ich mir an, wie gut ist denn mein Modell in diesem Fall? Also wie gut bildet es die bekannten Labels, die ich weiß, halt entsprechend ab?
 Das Ganze kann ich dann irgendwie in einen sogenannten Loss packen und sagen, hey, hier hast du alles falsch gemacht, pass mal deine Gewichte an.
 Und das ist genau das, was in der Optimierung passiert. Also ich rechne einen Score aus, gehe dann in die Optimierung, gucke mir das an.
 Und date dann die Gewichte von meinem Modell entsprechend ab. Und nachdem ich das gemacht habe, fange ich wieder von vorne an.
 Die Data Preparation fällt dann halt entsprechend raus. Also ich schiebe einfach meine Daten wieder rein und habe eine neue Prediction, berechne einen neuen Loss.
 Und mache das Ganze entsprechend oft, bis ich gut bin, was auch immer das heißt.
 Genau. Wie ich schon gesagt habe, wir brauchen erstmal ganz viele Daten und diese Daten müssen entsprechend gelabelt sein. Und wenn wir das jetzt vergleichen mit klassischer Softwareentwicklung, dann haben wir da einen Shift.
 Normalerweise in der klassischen Entwicklung von Algorithmen, probieren wir, wir haben einen Input und wir wissen die Regeln und das Ganze probieren wir irgendwie in einen Algorithmus zu geben.
 Und produzieren damit ein Output und beim maschinellen Lernen dreht sich das ein bisschen um. Und zwar haben wir den Input und den Output für das supervised learning und wir probieren die Regeln zu lernen.
 Und das bedeutet im Endeffekt, die Regeln, die wir lernen, steckt in den Daten drin. Und deswegen muss ich halt auch so viel auf die Daten aufpassen und gucken, was da drin steht.
 Weil genau diese Regeln halt entsprechend gelernt werden und ich dadurch auch eine große Abhängigkeit von den Daten entsprechend habe.
 Das zweite ist, ich habe auch eine Intransparenz, weil ich nicht mehr so genau weiß, was für Regeln denn mein Modell gelernt hat.
 Und das kann entsprechend dann dazu führen, dass es nicht mehr kontrollierbar ist, welche Regeln da überhaupt angewendet werden.
 Also in dem Algorithmus selber passieren Dinge, die für Menschen nicht mehr verständlich sind.
 Genau, als nächsten Schritt, habe ich gesagt, müssen wir die Daten vorverarbeiten.
 Das ist vor allen Dingen ein Teil, der wichtig ist für das maschinelle Lernen, also nicht für das deep learning.
 Das ist da der entsprechend große Unterschied.
 Ich muss die Daten vorverarbeiten, cleaning, ich muss Feature Engineering machen.
 Feature Engineering bedeutet im Endeffekt mit menschlicher Expertise aus den Daten herauskitzeln, gewisse Charakteristika herauskitzeln,
 die für meinen einen Use Case tatsächlich entscheidend sind und entsprechend den Unterschied zwischen dem einen und dem anderen genau hinbekommen.
 Das kann zum Beispiel sowas einfach sein, eine mathematische Transformation.
 Das kann auch das Zusammenfassen von unterschiedlichen Variablen sein.
 Das kann auch eine Reduktion sein, weil zum Beispiel gewisse Daten miteinander zusammenhängen und dadurch stark korrelieren und ich dadurch gar nicht alle Daten brauche.
 Und dadurch wird es für den Algorithmus ein Stück weit einfacher, entsprechend zu lernen.
 Genau, dann haben wir noch den Punkt kategorische Daten.
 Kategorische Daten sind zum Beispiel Geschlechtseinteilungen.
 Also ich habe männlich, weiblich diverse und das Ganze muss ich irgendwie so abbilden, dass mein Algorithmus damit umgehen kann.
 Also ich muss das irgendwie in Zahlen repräsentieren.
 So, jetzt kann ich einfach sagen, ich mache 0, 1, 2, 3.
 Ist erstmal eine gute erste Idee.
 Da muss ich aber aufpassen, wenn ich jetzt Differenzen bilde zwischen 0, 1, 2, 3, dann ist die Differenz zwischen 0 und 1 ein oder andere als zwischen 0 und 2.
 Und ob das dann die Realität widerspiegelt, ist in dem Fall fraglich und genau solche Dinge muss ich entsprechend berücksichtigen, wenn ich meine Daten irgendwie zusammenfasse.
 Genau, als nächstes muss ich dann ein Modell wählen.
 Das sind jetzt tatsächlich die Modelle aus dem Machine Learning.
 Da gibt es unfassbar viele und wie gesagt, das sind statistische Modelle.
 Und ein Modell ist erstmal einfach eine parametrische Repräsentation einer mathematischen Funktion.
 Und ich lerne genau die Parameter, um meine Funktion anzupassen auf meine Daten.
 So, der Wahl von einem Modell ist aber immer auch einer, hat immer implizite Annahmen.
 Ich habe hier das Beispiel linearer Regression mitgebracht.
 Relativ einfach, einfach eine Summe über gesichtete Daten.
 Also diese A's, die da stehen, vielleicht zu erkennen sind, sind halt die Gewichte, die ich entsprechend optimieren würde.
 Und ich nehme meinen Input und multipliziere den einfach und summiere das Ganze auf.
 Aber die Annahme, die da drinsteckt, ist, dass der Zusammenhang, den ich lernen will, halt auch tatsächlich linear ist.
 Also ich kann damit nicht irgendwas lernen, was am Ende nicht linear ist.
 Und das ist halt der Punkt. Also ich muss immer gucken, wenn ich ein Modell wähle, bildet das tatsächlich den Zusammenhang,
 den ich denke, der vorhanden ist, auch in einer gewissen Funktion ab.
 Und nur weil ich etwas komplexer mache, heißt das nicht, dass dann mein Modell tatsächlich besser ist.
 Kann ja auch was anderes werden, also wenn mein Zusammenhang linear ist, warum sollte ich dann was anderes nehmen als eine lineare Regression?
 Also an der Stelle ist tatsächlich ein bisschen Vorsicht geboten und man muss einfach gucken, was habe ich denn für Ideen, die da drin sind
 und dann vielleicht auch einfach unterschiedliche Modelle ausprobieren und zu gucken, welche ist denn tatsächlich besser.
 Und wenn es tatsächlich sehr, sehr einfache Modelle sind, kann ich auch daraus Rückschlüsse ziehen, welche Daten, welche Inputs denn wirklich wichtig sind für meinen Algorithmus.
 Und dann kommen wir zu dem Unterschied zum Deep Learning. Deep Learning heißt so, weil es halt viele mathematische Operationen,
 sogenannte Layer hintereinander geschaltet sind. Und genau, der Aufbau davon nennt sich dann Architektur.
 Das ist die Architektur von meinem neuronalen Netz, den ich habe. Der ist erstmal frei wählbar.
 Also wie ich die Sachen, wie viele Layer ich dann nehme, wie ich die zusammenstecke und so weiter und so fort.
 Das kann ich frei entscheiden, das kann ich irgendwie machen und am Ende zählt nur, dass das Richtige bei rauskommt.
 Also das heißt, wenn ich jetzt anfange, irgendein neuronales Netz zu turnieren, muss ich halt auch am Ende gucken,
 hey, wie würde denn vielleicht ein neuronales Netz anders auf diesen Daten tatsächlich funktionieren? Ist es vielleicht besser oder nicht?
 Ist erstmal eine Frage, die ich so nicht beantworten kann. Diese Layer sind dafür da, um die Daten zu transformieren.
 Und das ist halt genau der Punkt, was das Feature Engineering macht.
 Vorher im maschinellen Lernen Feature Engineering war das, was Menschen tun.
 Hier ist es, beim Deep Learning ist es tatsächlich so, das macht das neuronalen Netz ein bisschen selber.
 Was da genau passiert, weiß ich nicht. Aber ich habe hier ein Beispiel mitgebracht.
 Das ist ein Bild, was klassifiziert werden soll. Und wenn man sich danach dann anschaut,
 wie die transformierten Daten nach der ersten Transformation aussehen.
 Und da können zum Beispiel so Informationen drin sein, wie Kandendetektoren oder Grauwertunterschiede oder irgendwie sowas.
 Das kann da noch drin sein. Aber je tiefer das neuronale Netz ist, desto weniger ist es eigentlich verständlich, was dann im Endeffekt da passiert.
 Und es ist zwar erstmal sehr, sehr schön, dass ich das Feature Engineering nicht mehr brauche, weil ich weniger machen muss, um tatsächlich was zu produzieren.
 Aber wie gesagt, ich verliere in dem Fall tatsächlich die Transparenz.
 Und hier habe ich das Ganze nochmal dargestellt. Man sieht immer diese tollen Grafen, wo irgendwelche Pfeile irgendwo hingehen.
 Das, was sich dahinter versteckt, ist tatsächlich eigentlich nur Multiplikation und Addition.
 Erstmal multipliziere ich meine Input-Daten mit entsprechenden Gewichten.
 Die habe ich hier grau dargestellt, deswegen sind sie nicht zu sehen.
 Hier ist ein Gewicht, hier ist ein Gewicht und hier ist ein Gewicht. Dann addiere ich nochmal ein bisschen was drauf.
 Und das, was daraus passiert, das schmeiße ich in eine nicht-lineare Funktion.
 So, und das mache ich einfach ganz oft. Das ist im Prinzip alles, was dahinter steht.
 Wenn man multiplizieren und addieren kann, ist man im Prinzip schon fast besser als ein normales Netz.
 Der Punkt hier war einfach nochmal zu sagen, okay, wir haben diese unglaublichen abgefahrenen Flussgrafen, die da irgendwie abgebildet werden.
 Dahinter verbirgt sich tatsächlich eigentlich relativ einfache Mathematik.
 Und dann kommen wir endlich zum Lernen. Was ist denn Lernen? Lernen ist genau diese Optimierung.
 Diese Gewichte, die ich vorher genommen habe, mit denen ich meinen Input multipliziert habe, das sind die Sachen, die ich immer entsprechend anpassen kann.
 Und das mache ich halt durch eine stochastische Optimierung.
 Ich lege erstmal ein Modell fest und eine Architektur. Dann initialisiere ich alle Gewichte erstmal durch, mit irgendetwas.
 Und zwar am besten nicht mit Null, weil wenn ich die alle mit Null initialisiere, kommt am Ende auch Null raus.
 Und dementsprechend kann nichts gelernt werden. Es ist auch nur auf eine Frage, was ist tatsächlich eine gute Initialisierung.
 Gibt es unterschiedliche Ideen zu?
 Dann habe ich vorher festgelegt eine Lossfunktion. Und diese Lossfunktion leite ich ab.
 Ich gucke in Richtung des Gradienten. Der Gradient sagt mir, wie verändert sich meine Funktion.
 Und ich gehe dann in die Richtung des negativen Gradienten. Und das ist im Prinzip wie Skifahren.
 Wenn ihr oben auf dem Hügel seid, wollt ihr runter ins Tal. Und dann geht ihr auch in Richtung des negativen Gradienten.
 Und ihr fahrt nicht volle Power runter, weil dann würdet ihr viel zu schnell werden, sondern fahrt in Zickzacklinien.
 Und das macht ihr, um den Gradienten auszugleichen, weil der nämlich in die Richtung des steilsten Abstiegs zeigt.
 Und deswegen fahrt ihr schön Schlanglinien, damit es ein bisschen länger dauert und damit ihr nicht so schnell werdet.
 Das macht man hier auch im Endeffekt.
 Einfach, weil wenn man komplett in Richtung des Gradienten gehen würde, dann würde man vielleicht auch einfach woanders rauskommen.
 Also man würde das Minimum einfach überspringen.
 So gesehen ist es so ein bisschen die Frage, inwieweit muss ich denn in Richtung des Gradienten gehen.
 Und das ist die sogenannte Lernrate.
 Die wähle ich als Parameter vorher.
 Und ist fraglich, wäre es eine gute Lernrate.
 Also auch da habe ich im Prinzip einen Freiheitsgrad, den ich auswählen kann und gucken kann, was passiert, wenn ich eine größere Lernrate nehme, wenn ich eine kleinere Lernrate habe.
 Und damit habe ich erstmal eine Möglichkeit, etwas zu finden.
 Es gibt tatsächlich in der Mathematik natürlich viel, viel tollere Algorithmen, um so eine Optimierung vorzunehmen.
 Aber die sind teuer, weil ich dadurch tatsächlich zweite Ableitungen berechnen muss.
 Das ist sehr, sehr schwierig, das auszuwerten.
 Deswegen nehme ich an dieser Stelle den Gradienten.
 Und das Ganze ist stochastisch, weil ich halt einen Anteil von meinen Trainingsdaten einfach nehme und meine Trainingsdaten mir nicht den kompletten Raum aller Möglichkeiten abbilden können.
 Und auch da ist es so, ich kann unterschiedliche Zusammensetzungen meiner Trainingsdaten nehmen, um halt diesen Gradienten entsprechend auszuwerten.
 Das ist aber im Prinzip die ganze Magie, die dahinter steht.
 Ich gehe einfach quasi, ich stehe oben auf einem Hügel und gehe langsam runter in einen Tal.
 Und das Problem, ein großes Problem von diesem Maschinellernen ist, dass diese Hügellandschaften im Endeffekt nicht ein Minimum haben, sondern unterschiedliche.
 Und das sehr stark davon abhängt, welche Parameter ich wähle, in welchem Minimum ich lande.
 Und das weiß ich vorher nicht.
 Ich kann das nicht vorher sagen, ok, ich muss das machen oder das, um dann tatsächlich da zu landen.
 Und es kann auch sein, dass ich nicht im globalen Minimum, also in dem Minimum aller Minimal lande, sondern in einem Lokalen, was vielleicht schlechter ist als das globale.
 Aber mein Algorithmus kommt da nicht mehr raus und so gesehen lerne ich dann etwas, was ok ist, aber was nicht perfekt ist.
 Genau.
 Hier habe ich das Ganze nochmal dargestellt und zwar als, was ich vorher gesagt habe, ich muss ja gucken, dass ich irgendwie gut generalisiere.
 Das heißt, ich gucke mir hier einmal auf der linken Seite den Loss an von so einem Training.
 Und das sind halt diese blauen Punkte.
 Da sehen wir auf den Trainingsdaten, das sind die blauen Punkte und die durchgezogene Linie ist das Ganze auf den Testdaten.
 Und obwohl mein Algorithmus eigentlich immer besser wird, also unten sind die Iterationen meines Optimierungsalgorithmus aufgezeigt.
 Und obwohl mein Modell quasi immer besser wird auf den Trainingsdaten, also immer besser wird, immer meine Trainingsdaten immer besser vorhersagen kann, wird es halt schlechter auf den Testdaten.
 Das heißt, ich würde hier ungefähr da, wo dieser Knick von den Testdaten ist, würde ich aufhören und sagen, ok, hier ist eigentlich das beste Modell in diesem Fall erreicht.
 Und da würde ich das Training dann entsprechend abbrechen und dieses Modell nehmen.
 Und das Ganze habe ich nochmal rechts aufgezeigt auf der Genauigkeit.
 Das ist halt ein anderer Parameter, den ich messen kann.
 Und da sehe ich genau eben diesen Effekt.
 Obwohl im Training die Genauigkeit immer besser wird, auf den Trainingsdaten, wird sie halt auf den Testdaten immer schlechter.
 Und das ist dann halt ein Indikator dafür, dass das Training nicht so gut läuft, wie ich mir das eigentlich vorgestellt habe.
 Genau, ich kann unterschiedliche Trainingsmethoden miteinander kombinieren.
 Das ist ungefähr das, was tatsächlich passiert in den Large Language Models.
 Also ich kann erstmal irgendwie was lernen.
 Auf der linken Seite sieht man ein riesengroßes Monster, was erstmal probiert, irgendwelche Zusammenhänge zu lernen zwischen irgendwelchen Datenpunkten,
 die irgendwie vielleicht was miteinander zu tun haben.
 Das Ganze mache ich vielleicht auf irgendeinem Datensatz, der jetzt auch nicht super gut kuratiert ist,
 sondern erstmal nur da ist, um erstmal so eine Vorinitialisierung, eine gute Vorinitialisierung von meinem Modell zu bekommen.
 Und im nächsten Schritt gehe ich dann dahin und sage, ok, jetzt habe ich tatsächlich einen Datensatz, der ist gut kuratiert.
 Da ist das Richtige drin und darauf feintune ich dann mein vorinitialisiertes, geklostertes Modell.
 Und dann kann ich halt im letzten Schritt, und das ist zum Beispiel das, was man bei Chetchupti macht, da mache ich dann Reinforcement Learning in the Loop.
 Das heißt, ich habe einen Menschen, der interagiert mit diesem Computersystem.
 Das Computersystem macht unterschiedliche Vorschläge, guck mal, also der Mensch gibt eine Frage ein, dann generiert der Computer unterschiedliche Antworten
 und dann sagt der Mensch, das ist eine gute Antwort und das ist eine schlechte Antwort.
 Und dadurch kann ich dieses Modell nochmal mehr feintunen, darauf, dass tatsächlich das passiert, was der Mensch erwartet.
 Und dann der letzte Schritt, das ist ja das, was ich vorhin angedeutet habe, ich will eine gute Generalisierung haben von meinem Modell.
 Hier sehen wir, diese Generalisierung kriege ich immer nur hin, indem ich halt einen Testdatensatz habe, auf dem ich entsprechend gucke,
 wie gut sagt jetzt mein Modell auf diesem Datensatz etwas vorher.
 Und im Endeffekt ist das so ein bisschen so ein Trade-off, dieses Training zwischen Underfitting, also das heißt, mein Modell ist eigentlich zu schlecht auf meinen Testdaten
 und Overfitting, also mein Modell ist eigentlich viel zu gut auf meinen Trainingsdaten.
 Und irgendwie zwischen diesen beiden Welten schwank ich immer hin und her und probiere an der Stelle tatsächlich das Beste zu finden.
 Und hier ist halt auch nochmal ganz klar hervorzuheben, also ein Modell wird nie hundertprozentig perfekt sein, das ist eigentlich ausgeschlossen.
 Also wenn man den Fall hat, dann macht man wahrscheinlich irgendwas falsch.
 Der zweite Punkt ist, die Anwendung entscheidet darüber, welcher Fehler denn tatsächlich akzeptabel ist oder nicht.
 Also, die Frage war, was ist denn hier das Overfitting?
 Das Overfitting ist, dass das Modell auf den Trainingsdaten zu gut ist und im Endeffekt auf den Testdaten schlecht wird.
 Im Prinzip haben wir das auf diesem Plot, den wir vorher gesehen haben, wo ich die Lossfunktion her dargestellt habe und die Genauigkeit,
 da haben wir im Endeffekt ein Overfitting auf den Trainingsdaten gesehen. Das Modell ist immer besser geworden, obwohl es auf den Testdaten nicht mehr gut ist.
 Genau, also ein Modell wird nie hundertprozentig perfekt sein und alles richtig machen, dann ist man wahrscheinlich irgendwie auf dem Holzweg.
 Und der Fehler, den ich entsprechend akzeptieren kann, hängt halt von der Anwendung ab.
 Zum Beispiel, wenn ich eine Frauddetection machen will, dann muss ich irgendwie abwägen zwischen, ok, ich blockiere jetzt alles, was irgendwie komisch aussieht
 und dadurch habe ich dann wahrscheinlich auch sehr viele Nutzer, die nicht ihre normalen Transaktionen machen können, warum auch immer.
 Dann habe ich halt Nutzer, die genervt sind, die dann vielleicht zu einem anderen System oder zu einem anderen Konkurrenten gehen.
 Oder ich habe halt tatsächlich zu viel Fraud, also ich verliere irgendwie echtes Geld und dadurch habe ich als Firma einen finanziellen Verlust.
 Und dazwischen muss ich irgendwie gucken, für die Anwendung ist das ein Fehler, den ich akzeptieren kann oder nicht.
 Und dann muss man vielleicht als Firma sagen, ok, ein bisschen finanziellen Loss nehmen wir halt irgendwie hin, aber dafür nerven wir unsere Nutzer nicht zu sehr und die haben eine gute User Experience.
 Und was das Beste ist an der Stelle, ist einfach mathematisch nicht zu beweisen.
 Also der Unterschied zwischen der Performance auf dem Trainingsdatensatz und auf dem Testdatensatz, eigentlich habe ich da gar nicht eine wirkliche Idee, was denn da wirklich passiert.
 Also ich muss es wirklich ausprobieren und ich habe keine Möglichkeit zu sagen, ok, das ist tatsächlich das, das ist das mathematisch beste Modell.
 Das geht einfach nicht.
 Genau und ich habe nochmal ein Beispiel mitgebracht, wo ich tatsächlich auch nochmal diese Abhängigkeit von den Daten nochmal darstellen wollte.
 Also hier haben wir zwei, zwei dimensionale Daten, die ich entsprechend gelabelt habe.
 Hier oben haben wir einen Datensatz, dem geben wir die eine Klasse und hier unten haben wir einen Datensatz, dem geben wir die andere Klasse.
 So und jetzt möchte ich eine Funktion lernen, die mir dieses hier voneinander unterscheidet.
 Die Datenpunkte haben zwei Features, sind zwei dimensionale Datenpunkte und ein Label.
 Und ich nehme 80% meiner Daten als Trainingsdaten, 90% meiner Daten als Trainingsdaten und 10% als Test.
 Das lege ich einfach erstmal fest. So und dann nehme ich als allererstes mal 100 Datenpunkte und trainiere darauf und kriege eine Genauigkeit von 90%, was jetzt erstmal ganz gut ist.
 Aber im Endeffekt, wenn man sich jetzt nochmal überlegt, ok, ich nehme 10% meiner Daten als Testdaten, dann mache ich diesen Test tatsächlich auch nur auf 10 Datenpunkten.
 Also die Genauigkeit hier hängt tatsächlich, sind nur diese 10 Datenpunkte, davon sind 9 richtig und einer falsch.
 Und dann gehe ich halt nochmal hoch und sage ok, komm, wir nehmen einfach mal 1000 Datenpunkte, da haben wir dann eine höhere Genauigkeit von 98%
 und ich habe dann tatsächlich nochmal 10 Tantus-Datenpunkte genommen und die Genauigkeit verbessert sich an der Stelle dann nicht.
 Also eigentlich bin ich wahrscheinlich schon mit 1000 Datenpunkten erstmal ok, aber ich weiß erstmal, ich brauche zwar viele Daten, aber wie viel genau, weiß ich halt am Ende nicht.
 Und ich habe dann nochmal hier die Beispiele, also genau diese Klassifikationslinie dargestellt, die das Modell entsprechend lernt.
 Und das, was ich hier tatsächlich besonders interessant finde, ist, dass sich das Modell halt entsprechend verändert, je nachdem wie viele Datenpunkte ich habe.
 Also ich kann, wie gesagt, die Daten, die ich lerne, hängen halt tatsächlich extrem stark mit den, oder die Regeln, die ich lerne, hängen extrem stark davon ab, welche Daten ich habe.
 Und ich muss sehr viel Energie darauf verwenden, wirklich gute Daten zu erzeugen.
 Genau, also das war jetzt die Darstellung, wie lernen Maschinen eigentlich.
 Und wenn man jetzt anfangen will, dann kann man sich erstmal die Frage stellen, ok, brauche ich überhaupt maschinelles Lernen, um ein Problem zu lösen, oder tut es nicht was einfacheres?
 Und da denke ich, kann man ganz gut als ersten Indikator, ob ich jetzt wirklich maschinelles Lernen machen will oder nicht, diese drei Sachen sich anschauen.
 Das eine ist, wie viel Daten habe ich denn überhaupt zur Verfügung? Und komme ich leicht an Daten heran? Kann ich mir vielleicht selber Daten erzeugen?
 Oder kann ich die irgendwo relativ günstig herbekommen, vielleicht kaufen?
 Genau, und wenn ich eine große Menge an Daten habe, ok, dann kann ich den nächsten Schritt gehen und sagen, hat mein Problem überhaupt die entsprechende Komplexität?
 Oder tut es nicht was Einfacheres? Also für die Kaffeemaschine brauche ich keine Maschinelles Lernen, das ist Statistik, da zähle ich einfach durch.
 Und vielleicht mache ich noch einen Uhrzeitstempel dran und sage, ja, morgens trinkt er mal einen Kaffee und nachmittags immer ein Cappuccino.
 Dann habe ich das Problem eigentlich schon relativ schnell damit erschlagen.
 Und der letzte Punkt ist, habe ich eine Evaluierbarkeit? Also kann ich wirklich sagen, dass mein Modell besser ist oder nicht?
 Wenn ich das tatsächlich gar nicht habe, ist es halt schwierig zu argumentieren, dass mein Modell tatsächlich besser ist.
 Als Beispiel könnte man hier nehmen autonomes Fahren. Wie kann ich tatsächlich oder was ist erstmal der Grund, warum will ich das machen?
 Will ich das machen, weil ich es einfach kann? Ok, dann ist die Evaluierbarkeit relativ einfach und ich sage, ja, kann ich machen.
 Oder möchte ich zum Beispiel vermeiden, dass Menschen im Straßenverkehr ums Leben kommen?
 Und dann ist die Frage, wie evaluiere ich das? Brauche ich dann zwei Städte und in einem fahren dann autonome Autos und in dem anderen nicht und ich zähle am Ende?
 Das wird relativ schwierig, das halt zu argumentieren und zu sagen, ok, wir haben unser Ziel erreicht.
 Genau, zusammenfassend kann man einfach sagen, maschinelles Lernen ist im Prinzip ein wilder Mix aus Mathematik und Programmierung.
 Ich hoffe, ich konnte euch zeigen, es gibt sehr viele Parameter, die man einfach auswählen kann, wo ich auch nicht wirklich, also wo ich Intuitionen für brauche, wie ich die denn tatsächlich gut wähle.
 Und im Endeffekt weiß ich nicht so genau, was die Wahl dieser Parameter dann am Endeffekt tun.
 Also ich kann das nur ausprobieren und gucken, tun sie das, was ich auch wirklich erwarte oder nicht und muss es im Endeffekt hunderte Male irgendwie durchlaufen lassen, bis ich irgendwie sagen kann, das ist gut, das ist schlecht.
 Die Regeln sind in den Daten versteckt und es ist tatsächlich ziemlich schwierig zu verstehen, ob das Richtige gelernt wurde oder nicht.
 Und das ist dann tatsächlich in der Anwendung ziemlich schwierig, weil ich erstmal evaluieren muss, macht die Anwendung das, was ich haben will und gibt es vielleicht irgendwelche Edge Cases.
 Diese Edge Cases kann ich tatsächlich nur wahrscheinlich mit sehr viel Testing überhaupt herausfinden.
 Und dann, wenn ich dann einen Edge Case gefunden habe, den ich nicht haben will, dann kann es tatsächlich technisch ziemlich, ziemlich schwierig sein, diesen Edge Case wieder loszuwerden, weil ich nicht einfach sagen kann, ok, ich schmeiße diesen einen Datensatz jetzt aus meinen Trainingsdaten raus und dann wird das Problem behoben.
 Sondern eigentlich fange ich wieder von vorne an.
 Das heißt, in der Anwendung brauche ich im Prinzip eine konstante Überwachung, macht der Algorithmus eigentlich das, was ich will, gerade wenn ich sowas habe wie ein System, was automatisch lernt.
 Also es kommen irgendwie Daten rein und ich date das Ganze im Hintergrund, brauche ich einfach ein neues Modell, weil ich diese ganze Pipeline automatisiert habe.
 Dann kann es halt passieren, dass ich einen Data Shift habe und mein Modell das überhaupt gar nicht mehr abbildet.
 Ein bestes Beispiel ist dann irgendwie die Pandemie schlägt ein, alle Leute kaufen Klopapier und natürlich hat der Algorithmus das in den historischen Daten noch nie gesehen, dass wenn irgendwas draußen passiert, mit dem er gar nichts zu tun hat, plötzlich alle Leute Klopapier kaufen.
 Und das sind halt genau solche Sachen, die dann an der Stelle passieren können, wo ich ein spezielles Augenmerk drauf legen muss.
 Ja und ich meine, als letztes, also ich gehe davon aus, dass das maschinelle Lernen und das Deep Learning nicht mehr weggehen wird und ich glaube, deswegen ist es auf jeden Fall notwendig, dass wir uns alle mehr damit auseinandersetzen und das Ganze kritisch hinterfragen und gucken, wo sind die Dinge vielleicht nicht so, wie wir sie haben wollen, wo wollen wir überhaupt KI haben.
 Und im Endeffekt dann auch entsprechend sagen können, nein, an dieser Stelle macht es keinen Sinn, eine KI-Anwendung zu haben, weil sie nicht das tut, was wir haben wollen oder wir kann niemand mehr haben, der tatsächlich evaluieren kann, ob die Entscheidung, die ein Algorithmus trifft, auch wirklich valide ist.
 Und damit, wenn ihr Lust habt, euch noch mehr damit zu beschäftigen, ich habe am Samstag gebe ich noch einen Workshop zu dem Thema, wo ich mir das Ganze noch mal weiter aufdrösel und wir tatsächlich, also ich hoffe, dass ihr das dann alle macht, entsprechend mit einem neuronalen Netz ein bisschen rumspielt und damit irgendwie mal selber ausprobiert, was dann passiert, wann geht was kaputt und so weiter und so fort.
 Das wird am Samstag bei Jugendhackt um 14 Uhr stattfinden.
 Genau, ansonsten, wenn ihr dazu keine Lust habt oder was besseres vor habt, dann würde ich euch empfehlen, guckt euch mal Kaggle an.
 Kaggle ist tatsächlich eine Form, wo immer mal wieder Institutionen Datensätze hochladen und sich eine Community darum gebildet hat und man auch einfach sehr gut gucken kann, was machen eigentlich, wie gehen andere Leute diese Probleme an, die publizieren dann da auch entsprechend ihren Code.
 Man kann da einfach durch ein bisschen durchgucken, sich dann einfach neben, damit rumspielen, gucken, was passiert am Ende des Tages und so weiter und so fort.
 Als letztes habe ich noch ein paar Bücher, die tatsächlich auch mehr den Fokus auf diesen mathematischen haben, als jetzt auf dieser Anwendung.
 Und genau, damit vielen Dank für die Aufmerksamkeit.
 Vielen Dank, Chet.
 Wir haben noch fünf Minuten für Fragen, wenn du Fragen beantworten möchtest.
 Dann haben wir hier unten eigentlich ein Mikrofon im Publikum auch oder? Genau, also hat jemand Fragen? Fangen wir damit an.
 Ja, hier vorne, kann man das Mikrofon hier vorne mal durchgeben? Wir brauchen das Mikrofon, weil wir sind im Stream, sonst verstehen Leute im Stream gar nichts.
 Ja, danke schön. Ist vielleicht ein bisschen am Thema vorbei, aber könnte ich denn, um zu sagen, ich möchte so einen Zusammenhang, wie du gesagt hast, erst mal herstellen, also mathematisch, stellen wir uns vor, wir haben eine Industrieanlage und wir haben 10.000 Datensätze mit,
 sollte man das Ding reparieren. Jetzt sage ich, ok, Machine Learning, heißer Scheiß, das schmeiße ich mal drauf und stelle dann nachher fest, Mensch, mit Hilfe von Machine Learning stelle ich fest, welche Parameter wirklich relevant sind für, ich muss das Ding reparieren oder nicht, oder wäre der Machine Learning generell der falsche Ansatz?
 Also, ich würde sagen, wenn du Deep Learning machst, verlierst du halt entsprechend die Erklärbarkeit. Du kannst halt ein einfaches Modell nehmen, zum Beispiel eine lineare Version.
 Das ist halt natürlich vielleicht falsch, aber es gibt ja erstmal einen guten Indikator, der tatsächlich dir die hohe Erklärbarkeit an der Stelle erhält, weil du halt einfach ein Gewicht hast, mit welcher, was weiß ich, der wird halt, dieser Datenpunkt wird halt mit einem höheren Gewicht multipliziert
 und so gesehen ist das höchstwahrscheinlich ein Indikator dafür, dass das sehr entscheidend ist, um das vorherzusagen. Also, das Thema Erklärbarkeit, alle wollen natürlich erklärbare KI, aber ich würde sagen, geh lieber in ein Modell, von dem du vorher weißt, das ist erklärbar und damit kann man dann auf jeden Fall wahrscheinlich schon erstmal diese Sachen entsprechend lernen und hoffentlich auch besser verstehen.
 Vielen Dank. Gibt es mehr Fragen? Ja, ich sehe schon Leute stehen.
 Genau, Dankeschön. Vielleicht eher ein Kommentar. Ist so ein bisschen angeklungen, aber ich finde es immer noch wert, das nochmal zu betonen.
 In dem Moment, wo wir sozusagen den Schritt von den einfacheren, du hast es statistische Methoden genannt, hin zu den komplexeren, insbesondere zu denen, die kein Feature Engineering brauchen, machen.
 Wir bezahlen da ja nicht nur, indem wir Komplexität bekommen, sondern in der Regel bezahlen wir auch, indem wir viel, viel mehr Daten brauchen, von den guten Daten setzen, bis wir diese Modelle überhaupt stabil fitten können.
 Das ist glaube ich, was man sich auch nochmal sehr zu Gemüte führen sollte, wenn man sich da dran setzt. Und im Umkehrschluss ist es auch gut, im Hinterkopf zu behalten.
 Wir haben im Zweifelsfall sogar aus der klassischen Statistik eine ganze Menge Modelle zur Verfügung, von denen wir wissen, wie die sich verhalten.
 Und wenn ich jetzt Probleme habe, von denen ich überhaupt keine Ahnung habe, wie das ist, sondern wo es Experten gibt aus der Anwendung, die eigentlich schon ziemlich viel wissen darüber.
 Also in meinem Fall ist es zum Beispiel so, dass ich über meine Daten sage, die Physik und die Chemie eigentlich sehr viel aus.
 Und das kann ich ausnutzen, indem ich meine Modellarchitektur geschickt wähle. Und das wiederum macht sich dann ganz stark bezahlt,
 nämlich entweder bessere Modelle bekommen kann, ohne sozusagen die unnötige Komplexität oder indem ich mit weniger Trainingsdaten,
 und das ist irgendwas zwischen überhaupt möglich und teuer, zu brauchen. Danke.
 Auf jeden Fall. Dem kann ich nur zustimmen.
 Das war keine Frage. Vielleicht gibt es eine Frage noch. Also der Mensch, der hinter dir steht, glaube ich, würde ich da eine Frage stellen. Genau. Nein? Okay.
 Ja, ich habe auch nochmal eine Frage. Dennis Wolten, hallo. Ich fand den Punkt sehr interessant beim Deep Learning,
 wo du auf einmal geschmunzelt hattest und meintest, ja, da wissen wir aber auch nicht mehr, was da passiert.
 Im Kontext des Titels, die Mr. Firehanger, ja ey, muss ich da sehr lachen. Kannst du, also in meinem Verständnis habe ich schon eine grobe Ahnung,
 weil im Endeffekt sind da sehr viele Layer, sehr viele Modelle, sehr viele Berechnungen miteinander, aber kannst du da nochmal tiefer drauf eingehen und versuchen zu sagen, was da passiert, auch wenn du es nicht sagen kannst?
 Also das Thema Transparent ist ja, also ist auch ein hiebes Thema, gerade auch in dem Forschungsbereich.
 Da gibt es viele Leute, die sich da auch Gedanken drum machen, werden DFG-Anträge geschrieben, um genau das hinzubekommen, deine Erklärbarkeit hinzubekommen.
 Und also für so ganz einfache Neuronanenetze mit einem Layer, super, da kann ich irgendwie eine mathematische Theorie drum bauen,
 kriege ich alles irgendwie super gut beschrieben, aber sobald ich mehr Layer habe, geht mir diese ganze mathematische Theorie verloren.
 Und es wird wahrscheinlich eine Frage der Zeit sein und auch eine Frage, ob überhaupt man tatsächlich diese mathematische Theorie, um das verstehen zu können, nachzubauen.
 Und der zweite entscheidende Punkt an der Stelle ist, selbst wenn ich das Modell nachher komplett verstanden habe, habe ich immer noch eine Unsicherheit in den Trainingsdaten.
 Die Trainingsdaten bilden nicht den kompletten Raum ab, wobei es immer nur eine Teilabbildung von großem und ganzem ist.
 Und was da drin steckt, muss ich mir halt eigentlich händisch angucken.
 Und dafür will ich ja genau gerade die KI benutzen, um mir das zu erklären. Also so gesehen laufe ich da im Kreis.
 Also ich glaube, alle Leute wollen transparente KI, aber ich glaube, ich stehe eher auf dem Punkt, dass es unmöglich ist, das Ganze zu erklären.
 Vor allen Dingen, wenn ich mir da Chat-GPT angucke mit mehreren Milliarden an Parametern, das ist nicht erklärbar, was da drin passiert.
 Vielen Dank. Ein sehr spannender Vortrag und wir sind am Ende unserer Zeit. Und in 15 Minuten findet hier einen Vortrag in englischer Sprache über Hacken von Starlink und Satellitenclustern des Final Frontier.
 Bis dahin könnt ihr gerne sitzen bleiben, könnt euch die Beine vertreten oder geht woanders hin.
 Die Sonne wird erträglicher, weil es immer tiefer geht. Der Regen, der versprochen worden ist, ist immer noch nicht da. Aber danke, dass ihr da wart.
 Vielen Dank.
 [Musik]
